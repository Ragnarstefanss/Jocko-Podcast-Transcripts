{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'episodes'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from slugify import slugify\n",
    "\n",
    "def retrive_file_name(file_name):\n",
    "    # get only the file name\n",
    "    return file_name.split(\"\\\\\")[-1].split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "def get_slugify(file_name):\n",
    "    # clean up the file name and also replace w -> with\n",
    "    new_file_name = slugify(retrive_file_name(file_name))#.replace(\"-w-\", \"-with-\")\n",
    "    # get the episode number\n",
    "    ep_number = new_file_name.split(\"-\")[2]\n",
    "    return new_file_name, ep_number\n",
    "\n",
    "def create_output_path(output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    return output_path\n",
    "\n",
    "create_output_path(\"episodes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Rename downloaded files (cleaning up titles) and put in a subdirectory for cleaned up titles for both caption and full_text files\n",
    "from file cleaning-titles.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DONE: CREATED CLEAN FILES IN  captions/clean/\n",
      "==================================================\n",
      "==================================================\n",
      "DONE: CREATED CLEAN FILES IN  full_text/clean/\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def create_clean_title(main_dir, extension=\".vtt\"):\n",
    "    output_dir = create_output_path(main_dir+\"clean/\")\n",
    "        \n",
    "    for file_name in glob.glob(r'{}*{}'.format(main_dir, extension)):\n",
    "        new_file_name, ep_number = get_slugify(file_name)\n",
    "        dest = output_dir+new_file_name+extension\n",
    "        shutil.copy2(file_name, dest)\n",
    "    print(\"==================================================\")\n",
    "    print(\"DONE: CREATED CLEAN FILES IN \", output_dir)\n",
    "    print(\"==================================================\")\n",
    "\n",
    "create_clean_title(main_dir=\"captions/\", extension= \".vtt\")\n",
    "create_clean_title(main_dir=\"full_text/\", extension= \".txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: For captions: Take in renamed files from previous step and clean their contents up by removing empty spaces and make it into one single line for time and what was said\n",
    "from combine_lines_from combine_lines_from_cleaned.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "dic = {}\n",
    "paths_to_files = \"./captions/clean/\"\n",
    "output_path = create_output_path(\"episodes/captions/\")\n",
    "\n",
    "def create_clean_caption_files():\n",
    "    for file_path in glob.glob(paths_to_files+\"*.vtt\"):\n",
    "        # get the episode number\n",
    "        file_number = file_path.split(\"/\")[-1].split(\"-\")[2]\n",
    "        # add file contents to a dict\n",
    "        with open(file_path) as f:\n",
    "            arr = []\n",
    "            for word in f.read().strip().split(\"\\n\\n\")[1:]:\n",
    "                arr.append(word.split(\"\\n\", 1)[1].replace(\"\\n\", \" \"))\n",
    "            ## add this to file with correct episode number\n",
    "            text_file = open(output_path+\"/\"+file_number+\".txt\", \"w\")\n",
    "            text_file.write(\"\\n\".join(arr))\n",
    "            text_file.close()\n",
    "            dic[file_number] = arr\n",
    "            \n",
    "create_clean_caption_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def create_episode_folder_full_text(main_dir=\"full_text/\", extension=\".txt\"):\n",
    "    output_dir = create_output_path(\"episodes/\"+main_dir)\n",
    "        \n",
    "    for file_name in glob.glob(\"./full_text/clean/*.txt\"):\n",
    "        new_file_name, ep_number = get_slugify(file_name)\n",
    "        dest = output_dir+ep_number+extension\n",
    "        shutil.copy2(file_name, dest)\n",
    "create_episode_folder_full_text()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create hyperlinks to each episode for the React website\n",
    "from create_hrefs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "from slugify import slugify\n",
    "import os\n",
    "\n",
    "def create_hrefs(main_dir=\"./captions/\", extension=\".vtt\"):\n",
    "      arr = []\n",
    "      dic = {}\n",
    "      # iterate through all files with extension (.vtt)\n",
    "      for file_name in glob.glob(r'{}*{}'.format(main_dir, extension)):\n",
    "            # clean up the file name and also replace w -> with\n",
    "            new_file_name = slugify(file_name.split(\"\\\\\")[-1].split(\"/\")[-1].split(\".\")[0]).replace(\"-w-\", \"-with-\")\n",
    "            # get the episode number\n",
    "            ep_number = new_file_name.split(\"-\")[2]\n",
    "            # our new title which has been cleaned up\n",
    "            new_title = \"Jocko Podcast Episode #\" + ep_number+ \" - \" + \" \".join(new_file_name.split(\"-\")[3:])\n",
    "            # save a href link to a dictionary\n",
    "            dic[int(ep_number)] = \"<a href='episode/\"+ep_number+\"'>\"+new_title+\"</a>\"\n",
    "\n",
    "      # have to sort the dictionary since items are not in order\n",
    "      for k, v in dict(sorted(dic.items())).items():\n",
    "            # save href to array in correct order\n",
    "            arr.append(v)\n",
    "      # add this to a single file with all links to episodes\n",
    "      text_file = open(\"episodes/hrefs.txt\", \"w\")\n",
    "      text_file.write(\"\\n\".join(arr))\n",
    "      text_file.close()\n",
    "    \n",
    "create_hrefs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create title files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "from slugify import slugify\n",
    "import os\n",
    "\n",
    "output_path = create_output_path(\"episodes/titles\")\n",
    "\n",
    "def create_title_files(main_dir=\"./full_text/\", extension=\".txt\"):\n",
    "      arr = []\n",
    "      dic = {}\n",
    "      # iterate through all files with extension (.vtt)\n",
    "      for file_name in glob.glob(r'{}*{}'.format(main_dir, extension)):\n",
    "            new_file_name, ep_number = get_slugify(file_name)\n",
    "            # our new title which has been cleaned up\n",
    "            new_title = \"Jocko Podcast Episode #\" + ep_number+ \" - \" + \" \".join(new_file_name.split(\"-\")[3:])\n",
    "            # add this to file with correct episode number\n",
    "            text_file = open(output_path+\"/\"+ep_number+\".txt\", \"w\")\n",
    "            text_file.write(new_title)\n",
    "            text_file.close()\n",
    "\n",
    "create_title_files()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create links to original video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "from slugify import slugify\n",
    "import os\n",
    "\n",
    "output_path = create_output_path(\"episodes/links\")\n",
    "\n",
    "def create_link_to_youtube_episode(main_dir=\"./full_text/\", extension= \".txt\"):\n",
    "      arr = []\n",
    "      dic = {}\n",
    "      # iterate through all files with extension (.vtt)\n",
    "      for file_name in glob.glob(r'{}*{}'.format(main_dir, extension)):\n",
    "            new_file_name, ep_number = get_slugify(file_name)\n",
    "            # our new title which has been cleaned up\n",
    "            new_title = \"<a href=''><img src=''></a>\"\n",
    "            # # add this to file with correct episode number\n",
    "            text_file = open(output_path+\"/\"+ep_number+\".txt\", \"w\")\n",
    "            text_file.write(new_title)\n",
    "            text_file.close()\n",
    "\n",
    "## only uncomment if you are absolutely sure since this replaces manual edits\n",
    "# create_link_to_youtube_episode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Summarize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import os\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "stopwords = list(STOP_WORDS)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "punctuation = punctuation + '\\n' #x\n",
    "\n",
    "def create_word_cloud(episode_number, text):\n",
    "  wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "  plt.figure()#dpi=1200)\n",
    "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "  plt.axis(\"off\")\n",
    "  # plt.tight_layout(pad=0)\n",
    "  #plt.show()\n",
    "  image = wordcloud.to_image()\n",
    "  output_path = create_output_path(\"episodes/wordcloud/\")\n",
    "  image.save(output_path+episode_number+\".jpg\")\n",
    "\n",
    "\n",
    "def get_episode_summary(main_dir=\"./full_text/\", extension=\".txt\", length=0.03):\n",
    "  for file_name in glob.glob(r'{}*{}'.format(main_dir, extension)):\n",
    "    text = open(file_name, \"r\").read()\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc] #x\n",
    "\n",
    "    word_frequencies = {} #x\n",
    "    for word in doc:\n",
    "      if word.text.lower() not in stopwords:\n",
    "        if word.text.lower() not in punctuation:\n",
    "          if word.text not in word_frequencies.keys():\n",
    "            word_frequencies[word.text] = 1\n",
    "          else:\n",
    "            word_frequencies[word.text] += 1\n",
    "\n",
    "    max_frequency = max(word_frequencies.values()) #x\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "      word_frequencies[word] = word_frequencies[word]/max_frequency #x\n",
    "\n",
    "    sentence_tokens = [sent for sent in doc.sents] #x\n",
    "\n",
    "    sentence_scores = {} #x\n",
    "    for sent in sentence_tokens:\n",
    "      for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "          if sent not in sentence_scores.keys():\n",
    "            sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "          else:\n",
    "            sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "\n",
    "    select_length = int(len(sentence_tokens)*length) #x\n",
    "    summary = ' '.join([word.text for word in nlargest(select_length, sentence_scores, key = sentence_scores.get)]) #x\n",
    "    \n",
    "    _, ep_number = get_slugify(file_name)\n",
    "    output_path = create_output_path(\"episodes/summarize/\")\n",
    "    text_file = open(output_path+\"/\"+ep_number+\".txt\", \"w\")\n",
    "    text_file.write(summary)\n",
    "    text_file.close()\n",
    "\n",
    "    # Creating a word cloud files\n",
    "    create_word_cloud(episode_number=ep_number, text=text)\n",
    "\n",
    "# # get summary of episode and also wordcloud\n",
    "# get_episode_summary(length=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c86ef8fbad68d3f40798ce8384cdfae8823452373d62f00a17e6dc7d50f2988"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
